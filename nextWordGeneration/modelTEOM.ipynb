{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611526a1-9cdb-417d-baf3-1793dc7b309f",
   "metadata": {},
   "source": [
    "DAY - 2 : ( Testing && ( Enchancement && Outputing Methods ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fa3c4-3a37-40d0-9c6b-9a85ca0d3ca6",
   "metadata": {},
   "source": [
    "LETS ENCHANCE THE OUT PUT AS A TYPE WRITER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9df5dff-436b-4578-b59f-8f798530bd40",
   "metadata": {},
   "source": [
    "LETS LOAD THE tokenizer AND nextWordPredictionModel WHICH IS SAVED BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902c5c5-dd68-41fa-ae54-a339df284b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132624f0-450c-440a-a45d-658e1f50cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenized.pkl\",\"rb\") as f2:\n",
    "    tokenizer = pk.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fd10b-f41c-49d0-bb6e-669b07af789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"nextWordPredictionModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39791009-3e2d-4ba1-bfdb-2a100391ceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[0, 0,0,260,2]], verbose=0) #Checking the model is working are not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79c49e-5ce5-4c67-a085-638e940814c9",
   "metadata": {},
   "source": [
    "LETS CREATE A FUNCTION FOR TYPE WRITER\n",
    "\n",
    "Here iam using same function to preprocess the input text, which is used in model training notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ac0a2-c2f2-4bd0-bc48-290a9a5876a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca319622-5825-4428-a609-dba87b976013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(xtext: str, nlp, max_sequence_len = 5):\n",
    "    resulted_tokens = []\n",
    "    text = xtext.strip().lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:    \n",
    "        if token.is_alpha == True  and  token.is_oov == False:\n",
    "            resulted_tokens.append(token.text)\n",
    "            \n",
    "    temp_len = len(resulted_tokens)\n",
    "    if temp_len > 5:\n",
    "        return resulted_tokens[temp_len - max_sequence_len:]\n",
    "    \n",
    "    return resulted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d67924-5e6e-4d25-bcde-8dc273a1c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_sequence(text: str, nlp = nlp, max_sequence_len = 5, flag = True):\n",
    "    \n",
    "    if flag:\n",
    "        print(\"Before process - \", text)\n",
    "    sequence_list = []\n",
    "    word_index_dict = tokenizer.word_index\n",
    "    \n",
    "    #Calling the above function which preprocesses the input text\n",
    "    tokens_list = get_tokens(text, nlp = nlp, max_sequence_len = max_sequence_len) \n",
    "    if flag:\n",
    "        print(\"After process  - \", tokens_list)\n",
    "    \n",
    "    for token in tokens_list:\n",
    "        try:\n",
    "            sequence_list.append(word_index_dict[token])\n",
    "        except KeyError as k:\n",
    "            print(\"ERROR    : This is a small model might be one of the word in provided text is not existed.. \")\n",
    "            print(\"THEY ARE : \", k.args)\n",
    "            return 0\n",
    "            \n",
    "    if len(sequence_list) < 5:\n",
    "        #The function accepts the 2d array as input sequence\n",
    "        padded_list = pad_sequences([sequence_list], maxlen = max_sequence_len, padding=\"pre\") \n",
    "        return padded_list\n",
    "    \n",
    "    return np.array([sequence_list]) #import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf093a-0750-40d3-b8b4-3328f22cc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Next_Word(word_sequence_to_predict):\n",
    "\n",
    "    let_list = model.predict(word_sequence_to_predict, verbose=0)\n",
    "    resulted_index = np.argmax(let_list[0])\n",
    "    \n",
    "    index_word_dict = tokenizer.index_word\n",
    "    return index_word_dict[resulted_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ce53d-5c7c-4eb6-9f0b-66781085ba3e",
   "metadata": {},
   "source": [
    "#### METHOD - 1 [ PredictNextWord ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d555c-4522-4039-9c3b-e904eea765cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random as ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9880af-af8c-4e54-a707-ef5c171b7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextWord(text_from_user: str):\n",
    "\n",
    "    predict_seq = get_word_sequence(text_from_user, nlp=nlp, flag=False)\n",
    "    try:\n",
    "        print(predict_Next_Word(predict_seq))\n",
    "    except:\n",
    "        print(\"\\nSomething went wrong...!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d2589-e13d-44b2-b65a-abc283071e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### METHOD - 2 [ GenerateWords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253a517-0478-4153-bd08-f3c976e04438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_words(text: str, limit = 10):\n",
    "    \n",
    "    text = text.strip()\n",
    "    try:\n",
    "        for _ in range(0, limit):\n",
    "            predict_seq = get_word_sequence(text, nlp=nlp, flag=False)\n",
    "            result = predict_Next_Word(predict_seq)\n",
    "            text = text + \" \" + result\n",
    "\n",
    "            print(text)\n",
    "    except:\n",
    "        print(\"\\nSomething went wrong...!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af131d-df81-4a61-b0c4-98d6546f0fee",
   "metadata": {},
   "source": [
    "#### METHOD - 3 [ TypeWriter ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "36daa632-bee0-4537-b68a-a13f65ac1094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeWriter(text: str, limit = 10):\n",
    "    \n",
    "    text = text.strip()\n",
    "    try:\n",
    "        for _ in range(0, limit):\n",
    "            predict_seq = get_word_sequence(text, nlp=nlp, flag=False)\n",
    "            result = predict_Next_Word(predict_seq)\n",
    "            text = text + \" \" + result\n",
    "\n",
    "        splited_text = text.split(\" \")\n",
    "        lenOFtokens = len(splited_text)\n",
    "\n",
    "        for i in range(1,lenOFtokens+1):\n",
    "            sec = ran.choice([ 0.05, 0.1, 0.3])\n",
    "            time.sleep(sec)\n",
    "            print(\"\\r\"+\" \".join(splited_text[:i]),end=\"\")\n",
    "    except:\n",
    "        print(\"\\nSomething went wrong ...!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ac730c45-6883-43d4-8bd0-c9b35b0814b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "getNextWord(\"can i get some water please \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a8da204b-a763-425c-84e8-5abca08d8c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love me again to\n",
      "love me again to stay\n",
      "love me again to stay in\n",
      "love me again to stay in the\n",
      "love me again to stay in the pink\n",
      "love me again to stay in the pink shades\n",
      "love me again to stay in the pink shades of\n",
      "love me again to stay in the pink shades of the\n",
      "love me again to stay in the pink shades of the desert\n",
      "love me again to stay in the pink shades of the desert and\n"
     ]
    }
   ],
   "source": [
    "generate_n_words(\"love me again\", limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "90dbda51-9236-4fe9-8d93-859d317bb100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mokey one can imagine these two covered with sand running up the little street in the bright sunlight and still tools"
     ]
    }
   ],
   "source": [
    "print()\n",
    "typeWriter(\"mokey\", limit = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70443ecc-8fc6-4a99-8de7-55065bd14f4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### YOU CAN TEST THESE PROMPTS\n",
    "    - monkey\n",
    "    - donkey\n",
    "    - love me again\n",
    "    - love me like you do\n",
    "    - what are you doing now then\n",
    "    - tell\n",
    "    - tell me\n",
    "    - i want to be a\n",
    "    - i want\n",
    "    - i am\n",
    "    - i\n",
    "    - let me do one thing\n",
    "    \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_use]",
   "language": "python",
   "name": "conda-env-tf_use-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
