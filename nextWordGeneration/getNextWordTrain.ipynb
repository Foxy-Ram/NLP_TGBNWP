{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238d69ff-6847-4300-8253-85aca4d5fa48",
   "metadata": {},
   "source": [
    "NEXT WORD PREDICTION USING NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d029e0aa-6c2d-43c8-a0d4-6a5048a3a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e399f155-5c4b-4592-8202-40278024c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.utils import to_categorical, pad_sequences \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d56d67d-4b79-4acf-87b2-8aa58504aa56",
   "metadata": {},
   "source": [
    "LOADING DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0d5bb3-75d6-4699-9ca2-9b72baa60ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\kalyanram\\OneDrive\\Desktop\\DATA HUB\\archive\\generalEnglishText.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fa5f2e5-41ce-4496-93c6-e7aa825a19c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195776, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44768c10-301a-40e9-8738-2cab5a0f88fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', 'text', 'up_votes', 'down_votes', 'age', 'gender', 'accent',\n",
       "       'duration'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f0ac18-1f57-429d-96a9-a7274a75bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cv-valid-train/sample-000000.mp3</td>\n",
       "      <td>learn to recognize omens and follow them the o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv-valid-train/sample-000001.mp3</td>\n",
       "      <td>everything in the universe evolved he said</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cv-valid-train/sample-000002.mp3</td>\n",
       "      <td>you came so that you could learn about your dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cv-valid-train/sample-000003.mp3</td>\n",
       "      <td>so now i fear nothing because it was those ome...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cv-valid-train/sample-000004.mp3</td>\n",
       "      <td>if you start your emails with greetings let me...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  \\\n",
       "0  cv-valid-train/sample-000000.mp3   \n",
       "1  cv-valid-train/sample-000001.mp3   \n",
       "2  cv-valid-train/sample-000002.mp3   \n",
       "3  cv-valid-train/sample-000003.mp3   \n",
       "4  cv-valid-train/sample-000004.mp3   \n",
       "\n",
       "                                                text  up_votes  down_votes  \\\n",
       "0  learn to recognize omens and follow them the o...         1           0   \n",
       "1         everything in the universe evolved he said         1           0   \n",
       "2  you came so that you could learn about your dr...         1           0   \n",
       "3  so now i fear nothing because it was those ome...         1           0   \n",
       "4  if you start your emails with greetings let me...         3           2   \n",
       "\n",
       "   age gender accent  duration  \n",
       "0  NaN    NaN    NaN       NaN  \n",
       "1  NaN    NaN    NaN       NaN  \n",
       "2  NaN    NaN    NaN       NaN  \n",
       "3  NaN    NaN    NaN       NaN  \n",
       "4  NaN    NaN    NaN       NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ad3a5ad-a96f-4309-b97c-2d3e50d71893",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text.values # The remaining columns are unnecessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1118258-a68c-4dc0-aadb-62d3dc7d71d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['learn to recognize omens and follow them the old king had said',\n",
       "       'everything in the universe evolved he said',\n",
       "       'you came so that you could learn about your dreams said the old woman',\n",
       "       'so now i fear nothing because it was those omens that brought you to me',\n",
       "       'if you start your emails with greetings let me be the first to welcome you to earth'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text[:5000] # There are 195776 rows so here iam selecting 5000 rows.\n",
    "text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8a47b-6a82-4e6e-95ef-fd08802f2192",
   "metadata": {},
   "source": [
    "TOKENIZING THE CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6655c8a-bb41-4089-b4e5-44f540df6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd951900-1e82-4306-aeca-c8da4948a41a",
   "metadata": {},
   "source": [
    "SAVING THE TOKENIZER OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb18baf0-ad3d-4b53-a948-7db073218d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokenized.pkl\",mode=\"wb\") as f:\n",
    "    pk.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9029033d-4c4f-4fa9-a8f6-e2793b487b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3906"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index) # Number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f87e30c-1498-48b0-90ab-a02f45955cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1489"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index[\"universe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b17911a0-3feb-4686-8b28-fe63f5501c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78ad600c-5730-4090-92d4-479bc969f408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[260, 2, 1067, 197, 3, 820, 56, 1, 69, 212, 14, 16],\n",
       " [120, 12, 1, 1489, 1068, 6, 16],\n",
       " [11, 100, 47, 9, 11, 86, 260, 24, 42, 392, 16, 1, 69, 253],\n",
       " [47, 84, 7, 424, 106, 88, 10, 8, 117, 197, 9, 295, 11, 2, 25],\n",
       " [53, 11, 640, 42, 1262, 18, 1263, 201, 25, 27, 1, 118, 2, 1264, 11, 2, 296]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41c6be5d-b5ef-449f-aada-7c483c7c0079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_sequences) # There are 30000 rows of text that converted into text sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c8a6dc-45cb-4b84-85e0-de3fe1c9247c",
   "metadata": {},
   "source": [
    "LETS BEGIN THE FEATURE BUILDING THAT'S SUITABLE TO NEXT WORD PREDICTION AND MODEL\n",
    "\n",
    "FOR AN EXAMPLE:\n",
    "\n",
    "Text in a row     -> My name is kalyan ram\n",
    "\n",
    "Sequence allotted -> [1] [2] [3] [4]  [5]\n",
    "\n",
    "PREDICTION SHOULD BE:\n",
    "\n",
    "     X            Y\n",
    "     [1][2]       [3]\n",
    "     [3][4]       [4]\n",
    "     \n",
    "     Number of words stamp in x is 2\n",
    "     \n",
    "     X            Y\n",
    "     [1][2][3]     [3]\n",
    "     [2][3][4]     [5]\n",
    "     \n",
    "     Number of words stamp in x is 3\n",
    "     \n",
    "     NOTE: The above is just an example but i used another type of technique for some cases\n",
    "           that depends on user input text sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71724760-c4cb-4467-8a65-4e116c6b2e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[260, 2]\n",
      "[260, 2, 1067]\n",
      "[260, 2, 1067, 197]\n",
      "[2, 1067, 197, 3]\n",
      "[1067, 197, 3, 820]\n",
      "[197, 3, 820, 56]\n",
      "[3, 820, 56, 1]\n",
      "[820, 56, 1, 69]\n",
      "[56, 1, 69, 212]\n",
      "[1, 69, 212, 14]\n",
      "[69, 212, 14, 16]\n"
     ]
    }
   ],
   "source": [
    "# This is the method\n",
    "var = [260, 2, 1067, 197, 3, 820, 56, 1, 69, 212, 14, 16]\n",
    "def example (max_n_stamps = 4):\n",
    "    temp = 0\n",
    "\n",
    "    for i in range(2,len(var)+1):\n",
    "        if len(var[:i]) > max_n_stamps:\n",
    "            temp += 1\n",
    "            print( var[temp:i] ) \n",
    "        else:\n",
    "            print( var[:i] )\n",
    "example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1bc2ba-d648-43f9-94de-27632c88732c",
   "metadata": {},
   "source": [
    "BELOW IS FOR EXTRACT AND ADJUST THE SEQUENCES FOR ALL THE ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb077502-8de8-44cb-a9f7-26b82db430d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_stamps = 6\n",
    "adj_sequence = []\n",
    "for text_sequence in text_sequences:\n",
    "    temp = 0 #The temp variable should be zero for every sequence\n",
    "    \n",
    "    for i in range(2,len(text_sequence)+1):\n",
    "        if len(text_sequence[:i]) > max_n_stamps:\n",
    "            temp += 1\n",
    "            adj_sequence.append( text_sequence[temp:i] ) \n",
    "        else:\n",
    "            adj_sequence.append( text_sequence[:i] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8b72faf-bf55-45a5-bff2-398fb9bcdd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42318"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adj_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cbef44-97f9-40d3-af69-3f4e8fe03366",
   "metadata": {},
   "source": [
    "LETS USE PADDING TO GET THE ALL SEQUENCES INTO SAME SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abaa925b-7d36-4977-b7a7-2d1b43f8ba54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the padding maxlen is max_n_stamps of the sequence that you decided above\n",
    "# You can increase the maxlen more than the size of max_n_stamps \n",
    "# By changing the below variable\n",
    "\n",
    "increment = 0\n",
    "max_input_len = max_n_stamps+increment\n",
    "\n",
    "resulted_sequences = pad_sequences(adj_sequence, maxlen = max_input_len, padding = \"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36e37500-c55e-4a3c-9dcd-c27d9866823a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,  260,    2],\n",
       "       [   0,    0,    0,  260,    2, 1067],\n",
       "       [   0,    0,  260,    2, 1067,  197],\n",
       "       ...,\n",
       "       [   0,    0,    0,    0,   44,   76],\n",
       "       [   0,    0,    0,   44,   76,    2],\n",
       "       [   0,    0,   44,   76,    2,   56]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulted_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85ae7a40-ec70-43ab-8ee7-4d3992187172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> | 42318\n"
     ]
    }
   ],
   "source": [
    "print(type(resulted_sequences) , \"|\",len(resulted_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b55a3b-9543-4c13-aef2-e7b9bf4f906b",
   "metadata": {},
   "source": [
    "SEPERATING THE X & Y'S DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef1fc0cd-9dbd-485b-af62-6975daa05a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resulted_sequences[:, :-1]\n",
    "y = resulted_sequences[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8b2e69a-eafe-43d3-ae27-f56fbcf2c1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,  260],\n",
       "       [   0,    0,    0,  260,    2],\n",
       "       [   0,    0,  260,    2, 1067],\n",
       "       [   0,  260,    2, 1067,  197],\n",
       "       [ 260,    2, 1067,  197,    3],\n",
       "       [   2, 1067,  197,    3,  820]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ae5672c-5332-40f1-82a6-ad13ba6f7eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2],\n",
       "       [1067],\n",
       "       [ 197],\n",
       "       [   3],\n",
       "       [ 820]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3774d918-11c9-4431-934f-ddc5dd1aaf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size -  3907\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size - \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e61062-0b5f-46d5-9e5a-01b818be39fa",
   "metadata": {},
   "source": [
    "LETS  BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "843e0325-2556-4142-8b99-bb572ab591d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 100)            390700    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150)               150600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3907)              589957    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,131,257\n",
      "Trainable params: 1,131,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#Here we do -1 just because max_input_len is the value that before seperation of x & y \n",
    "model.add(Embedding(vocab_size, 100, input_length = max_input_len-1)) \n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(vocab_size, activation = \"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42f9b6b8-078c-4f6a-a4a4-a0a01d50558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = sparse_categorical_crossentropy,\n",
    "    metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1003d84-e7da-4ce6-bf7b-2d9c6ae3a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1323/1323 [==============================] - 23s 15ms/step - loss: 6.4101 - accuracy: 0.0757\n",
      "Epoch 2/10\n",
      "1323/1323 [==============================] - 18s 14ms/step - loss: 5.5151 - accuracy: 0.1221\n",
      "Epoch 3/10\n",
      "1323/1323 [==============================] - 19s 14ms/step - loss: 4.7174 - accuracy: 0.1797\n",
      "Epoch 4/10\n",
      "1323/1323 [==============================] - 18s 14ms/step - loss: 3.9741 - accuracy: 0.2724\n",
      "Epoch 5/10\n",
      "1323/1323 [==============================] - 19s 14ms/step - loss: 3.3422 - accuracy: 0.3694\n",
      "Epoch 6/10\n",
      "1323/1323 [==============================] - 19s 14ms/step - loss: 2.8301 - accuracy: 0.4514\n",
      "Epoch 7/10\n",
      "1323/1323 [==============================] - 18s 14ms/step - loss: 2.4140 - accuracy: 0.5214\n",
      "Epoch 8/10\n",
      "1323/1323 [==============================] - 18s 14ms/step - loss: 2.0734 - accuracy: 0.5838\n",
      "Epoch 9/10\n",
      "1323/1323 [==============================] - 18s 13ms/step - loss: 1.7957 - accuracy: 0.6371\n",
      "Epoch 10/10\n",
      "1323/1323 [==============================] - 20s 15ms/step - loss: 1.5644 - accuracy: 0.6818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153ce0325b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=10, workers=2, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "131a7bf6-3162-491f-a735-4f443996876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323/1323 [==============================] - 6s 4ms/step - loss: 1.3162 - accuracy: 0.7333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.316225290298462, 0.7332577109336853]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502fbc4-18bb-46a8-95d1-4372b6989cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"nextWordPredictionModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f5b203-6fb5-4a28-b739-5811374f06f3",
   "metadata": {
    "tags": []
   },
   "source": [
    " FOR HERE THE TRAINING IS FINISHED. LETS PERFORM SOME TESTS ON THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb49f6d-0e7b-4755-ba6e-e583ceebef2a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22c4e875-4643-4cc6-b7ac-7dad2a8d4425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a82b26bc-2498-4bd3-b52c-8f0beddd28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(xtext: str, nlp, max_sequence_len = 5):\n",
    "    resulted_tokens = []\n",
    "    text = xtext.strip().lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for token in doc:    \n",
    "        if token.is_alpha == True  and  token.is_oov == False:\n",
    "            resulted_tokens.append(token.text)\n",
    "            \n",
    "    temp_len = len(resulted_tokens)\n",
    "    if temp_len > 5:\n",
    "        return resulted_tokens[temp_len - max_sequence_len:]\n",
    "    \n",
    "    return resulted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6ac0c61-3f8a-4953-af58-ce479cd4c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_sequence(text: str, nlp = nlp, max_sequence_len = 5, flag = True):\n",
    "    \n",
    "    if flag:\n",
    "        print(\"Before process - \", text)\n",
    "    sequence_list = []\n",
    "    word_index_dict = tokenizer.word_index\n",
    "    \n",
    "    #Calling the above function which preprocesses the input text\n",
    "    tokens_list = get_tokens(text, nlp = nlp, max_sequence_len = max_sequence_len) \n",
    "    if flag:\n",
    "        print(\"After process  - \", tokens_list)\n",
    "    \n",
    "    for token in tokens_list:\n",
    "        try:\n",
    "            sequence_list.append(word_index_dict[token])\n",
    "        except KeyError as k:\n",
    "            print(\"ERROR    : This is a small model might be one of the word in provided text is not existed.. \")\n",
    "            print(\"THEY ARE : \", k.args)\n",
    "            return 0\n",
    "            \n",
    "    if len(sequence_list) < 5:\n",
    "        #The function accepts the 2d array as input sequence\n",
    "        padded_list = pad_sequences([sequence_list], maxlen = max_sequence_len, padding=\"pre\") \n",
    "        return padded_list\n",
    "    \n",
    "    return np.array([sequence_list]) #import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "16d8dc83-7439-4662-a35a-93348dab9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Next_Word(word_sequence_to_predict):\n",
    "\n",
    "    let_list = model.predict(word_sequence_to_predict, verbose=0)\n",
    "    resulted_index = np.argmax(let_list[0])\n",
    "    \n",
    "    index_word_dict = tokenizer.index_word\n",
    "    return index_word_dict[resulted_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919e817-6d52-4298-9e19-b75670e0fea9",
   "metadata": {},
   "source": [
    "NOTE : The model takes last 5 words of your sentence to predit, if any word in last 5 words is not existed in training vocabulary the model raises an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8830da1e-d52e-47c3-a78f-c289b6c07986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails\n"
     ]
    }
   ],
   "source": [
    "text_from_user = \"If you start your\"\n",
    "predict_seq = get_word_sequence(text_from_user, nlp=nlp, flag=False)\n",
    "try:\n",
    "    print(predict_Next_Word(predict_seq))\n",
    "except:\n",
    "    print(\"THE CODE BROKED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53c9f7-233e-4d3c-97de-7cca01f3f25d",
   "metadata": {},
   "source": [
    "THESE ARE SOME RESULTS THAT MY MODEL PROVIDED"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e06f9d39-207f-4a68-9c5c-9ee70e3c63fb",
   "metadata": {},
   "source": [
    "Examples to try:                      |  Results:\n",
    "    - The work is so hard when        |      - he\n",
    "    - What are you                    |      - doing \n",
    "    - What are you doing              |      - here \n",
    "    - If you start your               |      - emails\n",
    "    - i want to be a                  |      - linux\n",
    "Not to try:                           |\n",
    "    - he is fatty                     | \n",
    "    - he is my kin                    |\n",
    "    - crack me a joke for             | \n",
    "    - he is hilarious                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f79d10-1b5b-4dc3-b41c-b069a7de165e",
   "metadata": {},
   "source": [
    "TEXT GENERATION PERPETUATELY BY NEXT WORD PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "acaa7664-9baf-409f-bd70-9e7c5288e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_words(text: str, limit = 10):\n",
    "    \n",
    "    for _ in range(0, limit):\n",
    "        predict_seq = get_word_sequence(text, nlp=nlp, flag=False)\n",
    "        result = predict_Next_Word(predict_seq)\n",
    "        text = text + \" \" + result\n",
    "        \n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "d37c9803-f07b-4bbb-bec4-50aa09fb95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can i make a little\n",
      "can i make a little crowd\n",
      "can i make a little crowd of\n",
      "can i make a little crowd of about\n",
      "can i make a little crowd of about twenty\n",
      "can i make a little crowd of about twenty people\n",
      "can i make a little crowd of about twenty people surrounding\n",
      "can i make a little crowd of about twenty people surrounding the\n",
      "can i make a little crowd of about twenty people surrounding the huge\n",
      "can i make a little crowd of about twenty people surrounding the huge hole\n",
      "can i make a little crowd of about twenty people surrounding the huge hole had\n",
      "can i make a little crowd of about twenty people surrounding the huge hole had said\n",
      "can i make a little crowd of about twenty people surrounding the huge hole had said the\n",
      "can i make a little crowd of about twenty people surrounding the huge hole had said the monk\n",
      "can i make a little crowd of about twenty people surrounding the huge hole had said the monk about\n"
     ]
    }
   ],
   "source": [
    "generate_n_words(\"can i make a\", limit = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b7b55-45b1-4f5e-952e-321ad1168450",
   "metadata": {},
   "source": [
    "------------- ACCOMPLISHED THE TRAINING -------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34071a2b-2da0-41d8-81a6-f466677635bb",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_use]",
   "language": "python",
   "name": "conda-env-tf_use-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
